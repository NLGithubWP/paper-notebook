Retiarii: A Deep Learning Exploratory-Training Framework

# Current Problems

There are three ways to create a new model candidate.

![image-20220419161111915](imgs/image-20220419161111915.png)



Exploratory-training process is not supported well in current software.

1. **Search space**:  In current system, like pytorch/tf, we have to code up all variations of models in one jubmo model, and there is a control-flow to pick one during model construction. **The control-flow in this Jumbo model** make it hard to optimize in memory usage, operator fusion etc.

   ![image-20220419165437608](imgs/image-20220419165437608.png)

   

2. **Search Strategy**: Exploration strategy is responsible for 

   - deciding which models to instantiate and train. in which priority, and when to terminate. eg, Random Search, Grid Search, Heuristic-based search, Bayesian based. and reinforcement learning.
   - Mange execution of training, eg. stop train the bad-performance model, add more resources to model with good performance. Share weights of overlapped layers.

   The implementation of Search Strategy often tightly couples an exploration strategy with a specific model space, which incure 2 problems:

   - **poor reusable**:  it;s hard to resuse search strategy designed for one search space by other search space.
   - **hard to scale**: It's hard to cross-model training or distributed training with Multiple GPU. 

# Contributation

The paper's system clearly decouples model space from exploration strategy and enables system optimizations to speed up exploration process.

1. New programming interface ( Mutator abstraction ) to 

   - sepcify DNN model space for exploration.
   - specify exploration strategy to decides
     - order to instantiate and train model.
     - prioritize model training.
     - when to terminate training.

   The Retiarii use Mutator abstraction for above specification ( Search space and Srategy ). 

   - The Search Space = `A set of base models and mutators.`
   - The Search Strategy = which basse model and mutator to use +  `when` to apply mutators to the base model. 

   Each mutator is fine-grained, capture a logical unit of modification. Reusable and composable 

2. Offers just-in-time engine to instantiate model, manage training of instantiated model, gather information of exploration strategy to consume, execute the decision. 

3. Offers `cross-model optimizations` to improve overall exploratory training process by using correlation information.

## Evaluation result

1. reduce the exploration time of popular NAS algorithm by 2.57X
2. Improve scalability of NAS using weight sharing wiht a speed-up 8.58 X

# Mutator as the Core Abstraction

1. Rather than encoding modification in the complex jumbo model, the system import exisitng model from Tf/Pytorch as base model and apply mutator to generate some new model. 

   There are three Mutator in system

   - Input Mutator: mutate inputs of matched operators
   - Operator mutator: replace matched operator with other operators
   - Insert mutator: Insert new operators or sub-graphs

   The search space includes all base model and new model.

2. The system can reocrds the relation between models generated by different mutators based on same base model. eg,. for two instantiatios of the same base model, the nodes not modified by mutator are considered identical. 

   Relation can be used to optimize the multi-model training.

3. The mutator can be applied to any subgraph of the model. And create a new model instance.

   ![image-20220419175540302](imgs/image-20220419175540302.png)

# Retiarii Just-In-Time Engine

The engine will instantiate model on the Floy and manage the training of the model dynamically. 

`Inputs`: based models, mutators, policy describing exploration strategy.

`Execution`: Pick one base model and mutator to generate a new model using strategy.

Strategy can be

- context-free strategy: ramdom choice
- history-based strategy. etc
- customization choice.

The engine reocrds the mutation history, thus it knows which node are not modified and stay identical. So the engine can perform corse-model optimization like 

- common sub-expression elimination, 
- corss model operator batching
- NAS optimization

The optimized Data flow graph are then converted to stardard model format for existing DL framework to perform single-model optimization before training.

The strategy also responsible for 

- Launch training on new model, 
- monitor the training and collect results, 
- adjust training resource allocation
- Terminate training of less promising models.

![image-20220419182139203](imgs/image-20220419182139203.png)

# Cross-Model Optimization

## Optimization Opportunities

### Common sub-expression elimination

Compute the identical operations only once.

It can be applied to non-trainable operations such as data loading and preprocessing. since it's determinstic operation. while in training, weight is changing. 

**Evaluation**

![image-20220419213808406](imgs/image-20220419213808406.png)

### Operator Batching

Common operators with different inputs and weights can potentially be batched together and computed in a single operator kernel.

1. Two graph share multiple layers with **same weights** can be merging. As shown below.
2. Two operations with different weight can also be batched with special kernels like **grouped convolution, batch_matmul**. That can parallel compute on slices of an inout tensor.

![image-20220419230838890](imgs/image-20220419230838890.png)

![image-20220419184057269](imgs/image-20220419184057269.png)

group convolution:

![image-20220419232624659](imgs/image-20220419232624659.png)

### Weight sharing

Instead of training graph's weight from scratch, shared weight are inherited from other graphs to continue the training in this graph. And only the different node will has different wiehgt.

1. It can let user developer to annoatte operator weights they want to share.
2. it will identify the weight sharing-enabled operators in common subgraphs.

The system incure a new type of parallelism when constructing executable graphs.

![image-20220419225513233](imgs/image-20220419225513233.png)

The System build super-graph automatically, And we dont need to store the check-point to disk and then reload. 

**Evaluation**

![image-20220419225626822](imgs/image-20220419225626822.png)

## Executable Graph Construction

To exploit the above optimization, the system need to construct graphs from raw models.

The construction involves:

- Model merging,
- device placement of operators.
- Training parallelims

### Device placement

![image-20220419190028675](imgs/image-20220419190028675.png)

For DFGs sharing the same dataset and preprocessing, these common operators can be merged by common sub-expression elimination.

The system will test each model for few iteration and then sort them based on iteration time.  The system will then pack as many as model possible.

**Evaluation**

![image-20220419214307634](imgs/image-20220419214307634.png)

### Mixed paralelism for weight sharing.

The system use both data parallelims and model parallelism to train the network.

![image-20220419191852896](imgs/image-20220419191852896.png)

**Evaluation**

![image-20220419222635225](imgs/image-20220419222635225.png)

# Evaluation

## Main founding



1. The separation of model space and exploration strategy makes it easy for Retiarii to try different combinations.  Retiarii currently supports 27 popular Neural Architecture Search (NAS) solutions. Most of them can be implemented by the three mutator classes provided by Retiarii.

2. A number of micro-benchmarks show how Retiarii’s crossmodel optimizations greatly improve training efficiency.

3. Retiarii improves the model exploration speed of three NAS solutions by up to 2.58°ø, compared with traditional approaches.

4. Retiarii improves the scalability of weight sharing-based NAS solutions and brings up to 8.58°ø speed-up using the proposed mixed parallelism, compared with data parallelism.

## Micro benchmarks

### Shard data loading and preprocessing

We compare Retiarii with a baseline that runs each model independently without common sub-expression elimination.

### Operator batching

Insert a adapter layer to a pre-trained Mobile Net, and multiple mobile Net share the same weights, only adaptor is different.

![image-20220419184057269](imgs/image-20220419184057269.png)

![image-20220419214446724](imgs/image-20220419214446724.png)

Overall, Retiarii’s operator batching improves the aggregate throughput by 3.08°ø when batching 192 models, compared with the baseline that can only train at most 12 models together. Retiarii can batch more models than the baseline because it only has one copy of (fixed) weights from MobileNet. Only the memory for adapters is increased when batching more models

### Weight sharing

Compare three cases

1. weight is saved and loaded through files
2. weight is saved and loaded through object in memory
3. the system's super graph with cross-model optimization.

## Speeding up NAS

Using MnasNet, NASNet, AmoebaNet.

![image-20220419225009849](imgs/image-20220419225009849.png) 

![image-20220419222323121](imgs/image-20220419222323121.png)

1. Retiarii is substantially faster than the two baselines due to the cross-model optimizations

## Scale weight shared training

