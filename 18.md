Unifying and Boosting Gradient-Based Training-Free Neural Architecture Search

# Introduction

## **Current problems**

NAS has been proposed to design neural architectures automatically, but the search costs are unaffordable in resource-contrainted scenarios.

As a result, `training-free metrics` have been developed to realize training-free NAS and this method incur significantly reduced search costs. And the architecture selected by traing-free NAS has good transferability.

But unified theoretical analysis of these training-free metrics is lacking

1. Theoretical relationshios of `training-free metrics` are unclear.

   => it's hard to decide which one to use.

2. No theoretical explaination why NAS using training-free metrics performs well.

   => cannot provide user with interpretability 

3. May exist untapped potential in training-free NAS.

## Contribution (没理解)

Perform a unified theoretical analysis of gradient-based training-free NAS

1. Theoretically prove the connections among different gradient-based `training-free metrics`.
2. based on 1, the paper derives `generalization guarantees` for NAS and use them to provide interpretations for training-free NAS algorithms.
3. Exploit the unified theoretical analysis to develop a novel NAS framework named hybrid NAS to boost previous training-free NAS algorithms.

# Notations and BackGrounds





# Theoretical Analyses of Training Free NAS



# Hybrid Neural Architecture Search



# Experiments







