2020-01-15 [OSDI-2020] A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters

# Abstract & Introduction

## Problems

All-reduce and Parameter Server (PS) cannot fully utilize GPU/CPU heterogeneous resources. 

1. All-reduce: only GPU machines are involved.
2. PS: **Gradients** are sent to PS, which runs on CPU machines. Ps then run optimizer to update weight, finally, ps send updated weight to each worker.

*In theory*, PS can offer even better performance by utilizing additional CPU machines to aid the GPU machines. However, *in practice* all the existing PS have **inferior performance** for **multiple design reasons**.  As a result, all-reduce has better performance in practice. 

## Motivation

**How to design PS system such that the performance is as good as that in theory?** 

## Contributation

BytePS unifies the cases where PS or all-reduce is theoretically optimal, and **generalizes the optimality** to any given number of **GPU/CPU machines with different PCIe/NVLink configurations, with analytical proofs**

1. We design a new distributed DNN training architecture, BytePS, for heterogeneous GPU/CPU clusters. With spare CPU cores and network bandwidth in the cluster, BytePS can achieve communication optimality  for DNN training acceleration. BytePS provides a unified framework which includes both all-reduce and PS as two special cases.
2. We further optimize the intra-machine communication. We explain the diverse and complicated topology in GPU ma- chines and present the optimal strategy and principles.
3. We propose Summation Service, which accelerates DNN optimizers by keeping gradient summation running in CPUs, and moving parameter update, which is the more computation intensive, to GPUs. This removes the CPU bottleneck in the original PS design.







# Background



# Motivation and BytePS Architecture









# BytePS Communication Design











# Summation Service







# Implementation





# Evaluation



