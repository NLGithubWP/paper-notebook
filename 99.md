2020-01-15 [OSDI-2020] A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters

# Abstract & Introduction

## Problems

All-reduce and Parameter Server (PS) cannot fully utilize GPU/CPU heterogeneous resources. 

1. All-reduce: only GPU machines are involved.
2. PS: **Gradients** are sent to PS, which runs on CPU machines. Ps then run optimizer to update weight, finally, ps send updated weight to each worker.

*In theory*, PS can offer even better performance by utilizing additional CPU machines to aid the GPU machines. However, *in practice* all the existing PS have **inferior performance** for **multiple design reasons**.  As a result, all-reduce has better performance in practice. 

## Motivation

**How to design PS system such that the performance is as good as that in theory?** 

## Contributation

BytePS unifies the cases where PS or all-reduce is theoretically optimal, and **generalizes the optimality** to any given number of **GPU/CPU machines with different PCIe/NVLink configurations, with analytical proofs**

1. We design a new distributed DNN training architecture, BytePS, for heterogeneous GPU/CPU clusters. With spare CPU cores and network bandwidth in the cluster, BytePS can achieve communication optimality  for DNN training acceleration. BytePS provides a unified framework which includes both all-reduce and PS as two special cases.
2. We further optimize the intra-machine communication. We explain the diverse and complicated topology in GPU ma- chines and present the optimal strategy and principles.
3. We propose Summation Service, which accelerates DNN optimizers by keeping gradient summation running in CPUs, and moving parameter update, which is the more computation intensive, to GPUs. This removes the CPU bottleneck in the original PS design.







# Background



# Motivation and BytePS Architecture









# BytePS Communication Design











# Summation Service







# Implementation





# Evaluation




n-pairs of shoes, 

m dollars,

k kinds of brands, each pair of shoes belonging to a particular brand, the price is b, the value of c,



```c++
 18  struct node{
 19      int v; // price
 20      int w; // value
 21     node(int a,int b){
 22         v=a;
   					w=b;
 23      }
 24  };
 25 vector<node>V[105];
 26  int dp[15][10005];
 27  int n,m,k;
 28  int main(void)
 29  {
 30      int a,b,c;
 31      while(scanf(" %d%d%d ",&n,&m,&k)!=EOF)
 32      {
 33         memset(dp,-1,sizeof (dp));
 34          for(int i=0;i<=m;i++) dp[0][i]=0; //initialization 
 35          for(int i=0;i<=k;i++) V[i].clear();
 36          for(int i=0;i<n;i++){
 37             scanf(" %d %d%d ",&a,&b,&c);
 38             V[a].push_back(node(b,c));
 39          }
   
 40          for(int i=1;i<=k;i++){ // each type
 41              int n0=V[i].size();
 42              for(int j=0;j<n0;j++){ // each item under this type. 
 43                  for(int v=m;v>=V[i][j].v;v--){ 
 44                      if(dp[i][v-V[i][j].v]!=-1) //Current brand transfer status 
 45                         dp[i][v]=max(dp[i][v],dp[i][v-V[i][j].v]+V[i][j].w);
 46                      if(dp[i-1][v-V[i][j].v]!=-1) //Previous card transfer status 
 47                         dp[i][v]=max(dp[i][v],dp[i-1][v-V[i][j].v]+V[i][j].w);
 48                  }
 49              }
 50          }
 51          if(dp[k][m]==-1) puts(" Impossible " ) ;
 52          else printf(" %d\n " ,dp[k][m]);
 53      }
 54      return  0 ;
 55 }
```





## 最小化如下方程 

objectFunction = x1+x2+x3(x4+x5)

### 有3个条件

#### 条件1 

​	a/x1+ b/x2+ c/x3  (d/x4 + e/x5) < f

#### 条件2

​	x1+x2+x3 (x4+x5) < g

#### 条件3

- x1,x2,x3,x4,x5 都是大于0的整数
- a,b,c,d,e,f,g 是用户指定的，但是他们也是大于0 的整数



如果用户指定一组a,b,c,d,e,f,g. 

怎么不用暴力搜索可以求出 objectFunction 的最小值？ 

