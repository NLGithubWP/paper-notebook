---
title: Large Language Models
header-image: "img/postcover/post02.jpg"
categories: [A paper note]
layout: post
---

## LLM

Transformer-based neural network as language model.

Fine-tune is not additive, may break existing knowledge learned.

Promp engineering: few-shot prompting. 

# RAG pipeline

  ![image-20231207193742319](../../img/a_img_store/image-20231207193742319.png)

# LLaMA

![image-20231207195423513](../../img/a_img_store/image-20231207195423513.png)

Difference between LLaMA with Transformer.

- **internal covariate shift** make the training slower, thus, we need **layer normization** to avoid it.

  - Layer norm works since it devided the variance

  - Computing mean is costly, thus Root Mean Square Layer Norm (**RMSNorm**) avoid that.

- use relative position **representation**
  - add a distance between each two tokens

![image-20231207202854909](../../img/a_img_store/image-20231207202854909.png)

























