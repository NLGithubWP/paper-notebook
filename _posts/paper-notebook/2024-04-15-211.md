---
title: Efficient Memory Management for Large Language Model Serving with PagedAttention
header-image: "img/postcover/post02.jpg"
categories: [A paper note]
layout: post
---



LLM inference is memory-bound.

Batching requests can improve the throughput.

In real usage, 65% of memory is used to store model parameters and 30% for storing request states. where the states are mostly about the key-value tensors associated with the attention mechanism. (KV cache)

KV-cache per request for LLM serving is huge and grows and shrinks dynamically.

inefficient management leads to memory waste, such as fragmentation, and redundant duplication, e.g.,

- pre-allocate a contiguous chunk of memory with the requestâ€™s maximum length (e.g., 2048 tokens), but this can be a problem if the real request doesn't have so many tokens. Besides, the entire chunk is reserved during the request's lifetime, other shorter requests cannot utilize the reserved chunk.
- cannot share the memory between requests. LLM uses parallel sampling and beam search to generate outputs per request,



Only 20% memory is effectively used for current system.

## Contributions

This paper divides the KV cache into blocks. attention keys and values of a fixed number of tokens.

Request => process, Block -> page, Tokens => bytes.

- blocks are not necessarily stored in contiguous space, so KV is a more flexible way.
- external fragmentation is also avoided.
- memory sharing can happen at the granularity of the block

Then the paper proposes an attention algorithm based on such storage

Finally, the paper implements LLM based on such pageAttention.

## Methods

### PagedAttention

It partitions the KV cache of each sequence into KV blocks, each block contains key and value vectors for a fixed number of tokens.









