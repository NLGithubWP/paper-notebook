title: Attention Is All You Need
header-image: "img/postcover/post02.jpg"
categories: [A paper note]

https://github.com/hyunwoongko/transformer

Decoder uses Auto-regresisve model.

Masked multi-head attention: this is to prevent the t's output cannot includes the subsequent inputs.

Parameters: N and d

LayerNorm vs batchNorm

![image-20230509141907224](../../img/a_img_store/image-20230509141907224.png)

![image-20230509131836284](../../img/a_img_store/image-20230509131836284.png)

![image-20230509131849366](../../img/a_img_store/image-20230509131849366.png)

![image-20230509135430026](../../img/a_img_store/image-20230509135430026.png)