---
title: A Gentle Introduction to Graph Neural Networks
header-img: "img/postcover/post02.jpg"
categories: [A paper note]
layout: post
---


https://distill.pub/2021/gnn-intro/

# Introduction

## Usage

GNN can be used in fake news detection, traffic prediction, and recommendation systems, physics simulations.

## Contributions

The paper mainly introduces 3 parts

1. What kinds of data are most naturally phrased as a graph.
2. What makes graphs different from other types of data.
3. GNN models architecture.

## Graph

V Node, E Edge, U Global.

## data as Graphs

### Img to graph

Each pixel in the image can be a node, and connect to other nodes via an edge. 

![image-20220509225405041]({{ site.baseurl }}/img/paper-img/image-20220509225405041.png)

### Text to graph

![image-20220509225525554]({{ site.baseurl }}/img/paper-img/image-20220509225525554.png)

### Social network to Graph

![image-20220509230018695]({{ site.baseurl }}/img/paper-img/image-20220509230018695.png)

# Problems in Graph data

Node classification, link prediction, and clustering.

### graph level

Predict which graph has a specific property. Graph classification

### Node-level

Predict which party the node belongs to.

### Edge-level

Find the relation between two nodes.

# ML for Graph

**Represent graph to be compatible with NN.**

1. The graph has nodes, edges, global context, and connectivity.
2. Different forms of graphs must be recognized as the same inputs.

![image-20220509231010346]({{ site.baseurl }}/img/paper-img/image-20220509231010346.png)

## GNN

1. **A GNN is an optimizable transformation on all attributes of the graph (nodes, edges, global context) that preserves graph symmetries (permutation invariances)**

2. Build with Message passing
3. Adopt a graph as input, and produce an output graph. The **connectivity** of the input graph will not be changed.

![image-20220509231430353]({{ site.baseurl }}/img/paper-img/image-20220509231430353.png)

![image-20220509232315796]({{ site.baseurl }}/img/paper-img/image-20220509232315796.png)

In prediction, apply a linear classifier to each node. All nodes will share the weight of the classifier.

**Pooling**

If a node has no embedding, we could generate an embedding for it by adding all edge's embeddings around it. 

![image-20220509232707972]({{ site.baseurl }}/img/paper-img/image-20220509232707972.png)

We cloud also add node's embeddings to generate embeddings of one edge. 

![image-20220509232915763]({{ site.baseurl }}/img/paper-img/image-20220509232915763.png)

## Passing message

SImple GNN only considers the information of nodes and edges separately. 

"Passing message" can take the information of connectivity into consideration.

1. Edges to nodes

2. nodes to edges

![image-20220509233247174]({{ site.baseurl }}/img/paper-img/image-20220509233247174.png)

## Global representations. U 

Add a master node or virtual node, which connects all nodes and edges. 

![image-20220509234008576]({{ site.baseurl }}/img/paper-img/image-20220509234008576.png)

# Performance

Passing message in the whole graph has the best AUC.

![image-20220509235100502]({{ site.baseurl }}/img/paper-img/image-20220509235100502.png)

# Problems

1. GNN/GCN has a large network, and it's hard to optimize the computation. As a result, it's hard to compute in CPU/GPU.
2. GNN/GCN is sensitive to hyperparameters, sample methods etc.

