---
title: Serving Deep Learning Models with Deduplication from Relational Databases
header-img: "img/postcover/post02.jpg"
categories: [A paper note]
layout: post
---
- index
{:toc #markdown-toc}
# Questions

1. This paper says the decoupling between DB and model serving can incur high memory costs and high latency. So a natural solution is to combine Db and the serving system somehow. But the paper then mentioned reduction could reduce memory cost and latency. How suddenly is the reduction connected to DB??
2. 

# Introduction

## Background & Motivation

Model serving has some operational costs due to the **decoupling** between data management and model serving systems. For example

- Model serving requires loading the large model into memory. => **high memory cost**
- Transferring data features to the model serving framework incurs **high latency.** 

Thus, it's essential to investigate the **serving DNN model from RDBMS natively**.

Specifically, **tensor deduplication by using RDBMS** in multi-model serving systems can reduce **memory usage and inference latency** and thus must be resolved immediately. 

## Gap

Existing de-deduplication techniques used in RDBM, files, and tensors are not applicable to model serving from RDBMS since

- Lacking consideration of model inference accuracy
- Lacking enhancement of database storage functionalities to support inference and deduplication better.

## Challenge

1. How to leverage indexing to detect similar parameters that can be **deduplicated without hurting the inference accuracy**?
2. A database page can contain multiple tensor blocks. How do you pack tensor blocks into pages to maximize page sharing across multiple models and minimize the number of needed pages representing all tensors?
3. How to augment the **caching policy** to i**ncrease the data locality** for deduplicated model parameters so that **pages that multiple models need** have a **higher priority** to be kept in memory.

## Goal

The paper proposes a novel **RDBMS storage design** optimized for tensor and DNN inference workload.

Performance metrics:

- Reduction in storage size, 
- Speed-up in inference.
- Improved cache hit ratio. 

## Solutions Overview

- Tensor block index for fast duplication detection
- Packing distinct tensors to pages to minimize storage size.
- Deduplication-aware buffer pool.
  - Design a cost model for locality-aware page eviction; it gives each item priority.

# Technique details



