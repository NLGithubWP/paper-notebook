---
title: PipeDream Generalized Pipeline Parallelism for DNN Training.
header-img: "img/postcover/post02.jpg"
categories: [A paper note]
layout: post
---

- index
{:toc #markdown-toc}


# Question

1. 

# Introduction

## Background & Motivations

DNNs are getting larger and computationally expensive to train, thus requires parallel execution acorss multiple accelerators. 

Intra-batch parallelization: 

- Data parallelism with BSP/ASP. Where the worker receive the gradients. Limitations is mianly on communinication. 
- Model parallelism: limitations: under-utilizations of GPUs and unconvenience of the manually partitioning. 

Inter-batch parallelism:

- Pipeline parallelism. E,g. GPipe.

## Gap

Data-parallelization training suffer from high communication cost at large scale. E,g. when workers == 32, the communication cost takes 90% of total training time. 

## Goal

Propose an system using pipeline parallelism to enable faster DNN training by combining intra-batch parallelism with inter-batch parallelization.

- Overlaps the computations and communications of different inputs in a pipelined fasion. 
- Achieve **high hardware efficiency** with no ppeline stalls in steady state. 
- **High statistical efficiency** (number of iterations needed to reach a particular target accuracy) comparable to data parallelism using the same number of workers.

## Challenge

DNN training is bi-directional thus incurs some challenges for using pipelining. 

- Inject all minibatches in an epoch 
  - => cannot reach desired target accuracy since gradients are averages over all training samples. 
- Inject m minibatches and update weight every m minibatches. (GPipe) 
  - => reduce the hardware efficiency (many unused time unit for each hardware. )

# Technique details

## Solving Worker Partitioning

Slow stage bottnecks the model training throughput, and result in resource under-utilization.

Problem defination:

- How to partition the DNNs into multiple stages to minimize the time taken by the slowest stage.

Soution:

- Profiles the model and hardware resources: time of F/B pass, size the layer outputs, size of parameters.
- Propose a partition algorithm to ensure each stage completes at roughly the same rate, while trying to minimize communication across workers in a topology-aware way. It computes
  - partition of layer into each stage, 
  - replication factor for each stage, 
  - optimal number of mini-batches to keep training pipeline bust. 
- Partition algorithm also allows each stage is replicated (data parallelism) to reduce the time usage.

- 

## Worker scheduling

Problems:

- determine whether it should perform forward/backward/ tasks.
- determine how the minibatches be routed with replicated stages.

Solution:

- 1F1B scheduling. 

## Effective Learning

Problem:

- Forward on one minibatch using w0, and backward may updates on w1 (w0 is already updated by another mini-batch.)

Solution:

- Use **weight stashing** to avoid the mismatch between weights of a single stage/GPU.
  - Weight stashing cannot guarantee the consistency across stages. 
- Vertical Sync:
  - eliminates the potential inconsistency across stages.

# Evaluation

Macro-bechmarks

1. Speedups in Time-to-target-accuracy.
   - compared with data parallelism
   - compared with model parallelism, and hybrid parallelism
2. Reduce the overheads of communications without increasing memory usage.
   - compared with GPipe. 

Microbenchmarks

1. Optimizer
2. Memory Footprint. 
3. Communication overhead. 
4. Effect of pipeline depth

## 

## 







