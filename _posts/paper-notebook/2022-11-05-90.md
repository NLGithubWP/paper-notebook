---
title: Dorylus Affordable, Scalable and Accurate GNN Training with Distributed CPU Servers and Serverless Threads
header-img: "img/postcover/post02.jpg"
categories: [A paper note]
layout: post
---

[TOC]

# Question

1. Why is using serverless cheaper than using a CPU?

   Users not only pay for computing but also pay for unneeded resources. (storage)

2. 

# Introduction

## Background & Motivation

GNN family (e,g. GCN) has gained lots of success. It's essential to train GNN in an affordable, scalable, and accurate manner.

## Gap

Training GNN requires lots of GPUs, but:

- Using GPU in the cloud is costly. 
- GPU has limited memory, hindering scalability. 

CPU-based training and graph sampling are used to solve those two issues (high cost and poor scalability.)  But:

- CPU has poor parallelism computation and thus has poor efficiency. 
- Graph sampling incurs time overheads and reduces the accuracy of trained GNN.



## Challenge

Using serverless can scale with low cost, but it's challenging to adopt it to DNN training:

- How to make computation fit into lambda's limited compute resources: 
  - A lambda thread is too weak to execute a tensor kernel on large data. (large data => high FLOPs => longer time)
  - Breaking data into tiny mini-batches incurs high-data transfer overhead.
- How to minimize the negative impact of Lambda's network latency. 
  - one-thrid of time on communication. 

## Goal

This paper proposes **affordable, scalable, and accurate** GNN training. 

- Affordable: low-cost.
- Scalable: billion-edge graphs.
- Accurate: higher than the sampling-based method.

Details:

- Use serverless computing and CPU servers. 
  - It overcomes the above challenges by **dividing** the training pipeline into tasks and executing them with suitable resources. 
  - Graph operations => CPU
  - Tensor computation => Lambdas.
- Use the bounded pipeline asynchronous communication (BPAC) model to reduce communication overhead. 
  - Different tasks overlap each other.
  - Allow **asynchrony** in parameter update and data gathering process. And also bounds the degree of asynchrony.

# System Design and Contribution

GNN forward can be divided into four computation stages: **Gather, ApplyVertex, Scatter, and ApplyEdge.** 

The graph is partitioned into the edge-cut algorithm.

## Tasks and pipelining

This is about how to decompose tasks into fine-grained tasks such that 

- The computation can be fit in Lambda.
- Overlap the tasks with each other.

1. Fine-grained tasks => fix task into lambda.

- Graph computation (adjacency matrics) => on graph sever
- Tensor data computation => on Lambda to benefit massive parallelism

2. Pipelining enables overlapping such that the communication cost can be hidden. 

- Vertices are partitioned into groups called intervals. Each interval is computed using one thread in GSs.

## Bounded Asynchrony

s

## Lambda Management



s







![image-20221107213644718](../../img/a_img_store/image-20221107213644718.png)

# Evaluation

