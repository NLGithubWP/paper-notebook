---
title: Auto-Pytorch, Multi-Fidelity MetaLearning for Efficient and Robust AutoDL
header-img: "img/postcover/post02.jpg"
categories: [A paper note]
layout: post
---

![image-20220513144529017]({{ site.baseurl }}/img/paper-img/image-20220513144529017.png)

AutoNet [12],





# Introduction

## Motivation

AutoML system should have both hyperparameter tuning, and NAS.

1. HyperParameter Tunning below cannot scale. 
   - BlackBox Bayesian optimization
   - Evolutionary
   - reinforcement learning
2. There is no multi-fidelity benchmark on learning curves for the joint optimization of architectures and hyperparameters

## Contribution

1. Propose a system with automatically-designed portfolios of architectures & hyperparameters. ensembling.

   - Auto-PyTorch Tabular performs **multi-fidelity optimization** on a joint search space of architectural parameters and training hyperparameters for neural nets.
   - It targeted tabular data

   The system combines state-of-the-art approaches from **multi-fidelity optimization**, ensemble learning, and meta-learning for a data-driven selection of initial configurations for warm starting Bayesian optimization.

2. Use **multi-fidelity optimization**: tasks on cheaper fidelities (training only for a few epochs)

3. Introduce a new benchmark LCBench for studying **multi-fidelity optimization**

4. Experiment shows it is better than several other common AutoML frameworks: AutoKeras, AutoGluon, auto-learn, and hyperopic-learn.

# Some notes of Related Work

1. The design space for the current NAS is over-engineered, leading to very simple optimization tasks where even random searches can perform well.
2. MetaLearning can be used for warm-starting.
3. **BOHB combines Bayesian optimization (BO) with Hyperband (HB) and has been shown to outperform BO and HB on many tasks. It speedups of up to 55x over Random Search**

# Auto-Pytorch

The system implements and tunes the full DL pipeline, including data preprocessing, neural architecture, network training, and regularization. 

## Search Space

A large number of hyperparameters

1. preprocessing options (e.g. encoding, imputation) 
2. architectural hyperparameters (e.g. network type, number of layers)
3. training hyperparameters (e.g. learning rate, weight decay- p in regularization ).

The system can study on either 

1. Small-space: Funnel-shaped variant,  which contains only 2 search targets.
   - Requires a predefined number of layers.
   - A maximum number of units.
2. Full-space: allow to achieve SOA performance.

![image-20220513144502272]({{ site.baseurl }}/img/paper-img/image-20220513144502272.png)

## Multi-fidelity optimization

It uses BOHB to find well-performing configurations.

