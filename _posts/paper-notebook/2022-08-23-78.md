---
title: How Powerful are Performance Predictors in Neural Architecture Search
header-img: "img/postcover/post02.jpg"
categories: [A paper note]
layout: post
---

# Introduction

The papre compare 31 nas algorithms in 4 search spaces and 4 datasets. 

- The algorithm ranges from zero-cost, model-based, learning curve extrapolation, and weight sharing.
- Configs: 
  - 101+cifar10,  darts+cifar10
  - 201 + (c10, c100, imageNet), 
  - nas-bench-NLP + Penn TreeBank.

After those experiments, the paper try to 

1. Find which predictors have consistent performance across search space from three dimision's comparison
2. Analysis insights.
3. Find complementary perdictors and invesigate how to combine them 

# Experiments

## Effectiveness

Measure SRCC, Pearson, Kendall Tau. 

For prediction model, they train the model with 1k archs and then test on 200 archs. 

![image-20220824213922872](https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220824213922872.png)

Conclusion

1. **low query time + low initialization time, Jacob and Synflow preform well.** 
2. Zero-cost do not perform well on large space like DARTS
3. High init time + low quer time, performance predictors are best. 

## Efficiency

The papre combine 3 metrics from 3 different families. 

1. SoTL-E from learning curve methods
2. Jacob from zero-cost method
3. Both of above are used as **input feature** for a model-based predictor. (SemiNAS and NGBoost. )

![image-20220824220850473](https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220824220850473.png)

After this, the papre measure how to use those metrics to speed up the NAS. It mainly use two methids.

1. Predictor-guided evolution framework. 
2. Bayesian optimization. 

![image-20220824220538422](https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220824220538422.png)

This suggests that using zero-cost methods in conjunction with model-based methods is a promising direction for future study.

