---
title: Memory Efficient Pipeline-Parallel DNN Training
header-img: "img/postcover/post02.jpg"
categories: [A paper note]
layout: post
---


- index
{:toc #markdown-toc}
# Introduction

## Background & Motivation

GNN family (e,g. GCN) has gained lots of success. It's essential to train GNN in an affordable, scalable, and accurate manner.

## Gap

Training GNN requires lots of GPUs, but:

- Using GPU in the cloud is costly. 
- GPU has limited memory, hindering scalability. 

CPU-based training and graph sampling are used to solve those two issues (high cost and poor scalability.)  But:

- CPU has poor parallelism computation and thus has poor efficiency. 
- Graph sampling incurs time overheads and reduces the accuracy of trained GNN.

## Challenge

Using serverless can scale with low cost, but it's challenging to adopt it to DNN training:

- How to make computation fit into lambda's limited compute resources: 

  - A lambda thread is too weak to execute a tensor kernel on large data. (large data => high FLOPs => longer time)
  - Breaking data into tiny mini-batches incurs high-data transfer overhead.

- How to minimize the negative impact of Lambda's network latency. 

  - One-thrid of time on communication. 

    E.g., When the number of Lambdas it launches reaches 100, the per-Lambda bandwidth drops to 200Mbps.

## Goal

This paper proposes **affordable, scalable, and accurate** GNN training. 

- Affordable: low-cost.
- Scalable: billion-edge graphs.
- Accurate: higher than the sampling-based method.

Details:

- Use serverless computing and CPU servers. 
  - It overcomes the above challenges by **dividing** the training pipeline into tasks and executing them with appropriate resources. 
  - Graph operations => CPU
  - Tensor computation => Lambdas.
- The bounded pipeline asynchronous communication (BPAC) model reduces communication overhead. 
  - Different tasks overlap each other.
  - Allow **asynchrony** in parameter update and data gathering process. And also bounds the degree of asynchrony.

