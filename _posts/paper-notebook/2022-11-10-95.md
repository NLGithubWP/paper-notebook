---
title: Neural Architecture Search as Program Transformation Exploration
header-img: "img/postcover/post02.jpg"
categories: [A paper note]
layout: post
---


- index
{:toc #markdown-toc}
# Introduction

## Background & Motivation

Bringing DNNs into various hardware is an important research area.

There are two communities optimizing DNNs for commodity devices. 

- NAS: trade-off between size and acc
- Compiler: optimize the existing well-defined model by restructuring underlying tensor computations.

## Gap

Few works of NAS consider the hardware behavior but have problems:

- Having complex methods for predicting the search space.
- Having a fixed pipeline and missing powerful candidate architectures.
- Limited to selecting from a pre-designed list of CNN operations. 

## Goal

This paper combines neural architecture and compiler optimization in a unified framework to achieve NAS in hardware aware manner. 

- This paper recasts neural architecture search as program transformation exploration.

- combines the NAS and compiler optimizations.

# Details

This paper uses many operator transfers, such as bottlenecking, grouping, depthwise, etc, to reduce memory usage and speed up the computing process.

To verify the transformation is correct, for each model in the search space, they use training-free-model evaluation metrics to quickly check if the model's score after transformation is the same as the previous one. If the new model's score is lower than the previous one, then don't use it in a later process. 