*Cool*: a COhort OnLine analytical processing system

# Introduction

## Current Problems

Cohort analysis is about comparing different group of users, and grouping is based on events or time users start a service. 

Cohort steps:

1. `Find birth user cohort:` group users experiencing similar given events to a cohort.
2. `Segmentation:` for each cohort, assign the records into diverse segments. 
3. `Aggregation:` for each each segment, measure using a given aggregator. 

However, OLAP system (*MonetDB*,*Druid*) is inefficiency in processing complex cohort queries. Although they use `columnar architecture` to scale up or take advantages of `distributed processing` for `specific queries`. It is non-trivial to `integrate both of them together.` 

## Contributations

The paper propose Cool, a cohort OLAP system. it supports both OLAP and emerging cohort queries for data analytics with superb performance.

1. The system includes:
   Two operators for OLAP query (metaChunk selector, dataChunk selector).
   Three operators for cohort query (birth selector, age selector, cohort aggregator.)
2. The system design a sophisticated storage layout to optimize query processing and space consumption. It store precomputation results to boost OLAP execution.
3. Scale the system with HDFS and Zookeeper. The system can recover form failures.
4. Compare Cool with Druid, MonetDB in single-node setting. 
   Compare Cool with SparkSQL and distributed Druid in distributed setting.

Cool differs with Cohana in following:

1. Cool use more general defination for cohort queries. It employs a sequence of birth events to find the valid users.
2. Cool support conventional cube queries.
3. Cool can be distributed. 

# Related Work

Existing system can be divided into:

1. conventional OLAP systems built on top of DBS.
2. specifc query engines for specific query types.

## Conventional OLAP

### columnar architecture

Most of them use `columnar store` because 

1. OLAP query normally scan a large volume of limited set of columns, and `columnar store` can only load the required column. 
2. In columnar store, the scanned data can be further shrunked by **compression** such as dictionary encoding, run length encoding. Row re-ordering encoding, etc.

But the data insertion and point query are worse than `row-oriented` database.

So hybrid storage is proposed, it combines both row-oriened store and columnar store together within one system, eg,. MemSQL, SAP HANA, Oracle Database in-memory Hyper.

### Distributed processing

To handle enormous amount of data, OLAP system also supoorts distributed processing. such as Hive,  Qylin. But it normally depends on worker resources. 

## Emerging Query Engines

Point offers real-time response for iceberg quries.( prevailing type of query selecting a small number of records which satisfy some given conditions.) 

**Data structure/indexs**

`Data cube` is widely used for queries involving multiple dimensions from dataset.

`Start-Tree` is used to support OLAP queries.

**Compression**

New compression algorithms also proposed to improve the performance

1. PowerDrill use two-level dictionary compression schema to enable processing of trillion record cells in seconds.
2. Condensed cube reduce size of cube and imporive the computation time.

**Injection**

Spark used to handle stream queries. 
Shark further improves its performance by leveraging distributed memories.

# Single Node Architecture

Loader, controller, parser, planner, compressor and executor.

![image-20220223164904589](imgs/image-20220223164904589.png)

![image-20220223153540828](imgs/image-20220223153540828.png)

## Data Model

Each table is horizontally split into different partitions called cublets.

Each cublet consists of multiple chunks (data chunk, meta chunk, header, header offset)

## Storage Layout

**Chuncks**

- `DataChunk:` Cool transcripts columnar data into field inside DataChunk.
- `MetaChunk:` is used to store the meta data `for each dataChunk` within the same cublet, including number of contained fields and range of each field
- `Header`: store number of dataChunks.
- `Header offset`: store header's position.

**Field Types:**

- Range: integer, float, time,  
- world: string, event.

system has different storage plan for different filed type.

**Storage plan**

- For range filed: only keep max/min value
- For world field: apply double dictionary schema to save space. 

**Compression**

Firstly, each field is encoded to reduce space usage.

- Numbers are translated into a lock dict
- distinct string is put into a global dictionary and can be indexed by a unique number. 

Besides, compression is also used.

- bit packing and vectorization are introduced to deal with delta values and dictionary indexing numbers.

## Query Processing

**Two operators for OLAP query:**

1. MetaChunk selector: (schema of chunkD, MetaChunk M, predicate tree P) => traverse M with P => whether the chunk contains data.

2. DataChunk selector: find matched data records in query processing.

   (dataChunk R, Record Number N, predicate tree P) => bites B (indexs of records)

**Three operators for cohort query**

1. Birth selector: capture qualified users. (MetaChunk M, dataChunk, predicate tree P) => where the events in birth event sequence exist in scanning chunk.
2. Age selector: (dataChunk, predicate tree P, bitset B) => marking whether the user can be selected for aggregation in age.
3. Cohort aggregator: the records pass both above selector can be aggregated. 

**OLAP Processing Flow**

1. `Planner` => generate execution plan
2. Fetch a `cublet` from specified data source.
3. `MetaChunk selector` => find `cublet` with candidate values
4. Repeat form 2 unit find a `cublet`
5. `DataChunk selector` => find records.
6. `Aggregators` on the scanning result and group the results;
7. Repeat from 2 until all cublets are processed. 
8. `Compressor`: compress and store agggregate results

**Cohort Query Processing**

1. `Planner` => generate execution plan
2. Fetch a `cublet` from specified data source.
3. `MetaChunk selector` => find `cublet` with `birth event in sequence`
4. Repeat form 2 unit find a `cublet`
5. `Birth selector` => scan dataChunk to find users.
6. `Age selector` => calculate ages. 
7. `Aggregator`: aggregate cohorts from user. 
8. Repeat from 2 until all cublets are processed. 
9. `Compressor`: compress and store agggregate results

# Distributed System Architecture

<img src="imgs/image-20220224155711239.png" alt="image-20220224155711239" style="zoom:50%;" />

# Performance Evaluation

## Experiment setup

**Compare**

Compare with Apache Druid, (OLAP related query evaluation)

MonetDB (columnar analytical dataabse for cohort query evaluation)

SparkSQL/Distributed Druid(distributed processing)

**Matrix**

Query latency / compression ratio / memory consumption in processing. 

**Workloads**

TPC-H benchmark / medical dataset MED

**Queries**

Cohort Query: used to measure cohort analysis query processing

IceBerg Query (OLAP): used to measure performance of selector in COOL

Cube query

Composite Query

<img src="imgs/image-20220224161909225.png" alt="image-20220224161909225" style="zoom: 67%;" />

## Latency

![image-20220224162311451](imgs/image-20220224162311451.png)

*MonetDB*incurs more disk accesses during the processing compared to *Cool* 

And the frequent data swapping, from disk to memory slows down the system performance in terms of query latency.

## Processing Memory

<img src="imgs/image-20220224163016138.png" alt="image-20220224163016138" style="zoom: 67%;" />

First, *Druid* needs to maintain auxiliary system components in order to manage the data segments, which is complex and incurs high overhead. However, *Cool* exploits a relatively simple storage hierarchy and therefore eliminates such extra costs. 

Second, *Druid* must map the data from disk to memory during query processing even for the tiny dataset while for *Cool*, due to the highly `compressed` cublet structures, it may keep the entire dataset in memory for small datasets, which further leads to the performance gap between the two systems.

## *Multi-node Benchmark*

Employ `1 to 16 nodes to` evaluate the system performance

![image-20220224163329949](imgs/image-20220224163329949.png)

SparkSQL: merge of all the results, instead of the computation of each parquet partition, dominates the system performance.

