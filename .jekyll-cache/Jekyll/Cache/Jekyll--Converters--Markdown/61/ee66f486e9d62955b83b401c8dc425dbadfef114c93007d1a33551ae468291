I"ü<h1 id="introduction">Introduction</h1>

<h2 id="problems">Problems</h2>

<h3 id="model-variance-selection">Model variance selection</h3>

<p>Inference query various on service level objectives such as accuracy, latency, and cost.</p>

<p>To better answer the userâ€™s inference request, the ML platform has to select the</p>

<ol>
  <li><code>Right model</code>:  RestNet50, VGG16 (TensorFlow, PyTorch)</li>
  <li><code>Model optimizations </code>: TensorRT, TVM, XLA.</li>
  <li><code>Hyper-parameters</code>:  batch-size</li>
  <li><code>Suitable hardware platforms</code>: Haswell / SkyLake CPU, GPU, TPU, NPU.</li>
  <li><code>Auto-scaling configurations</code></li>
</ol>

<p>But the search space is huge. For example, 21 already-trained image classification models can generate 4032 model-variants on AWS-EC2 platform by:</p>

<ul>
  <li>Applying various model graph optimizers (TensorRT, TVM, etc)</li>
  <li>Optimizing for different batch sizes.</li>
  <li>Changing underlying hardware resources. (CPU, GPU)</li>
</ul>

<p>The 4032 model variants vary across different dimensions:</p>

<ul>
  <li>accuracies range from 56.6% to 82.5%</li>
  <li>model loading latencies range from 590ms to 11s</li>
  <li>inference latencies for a single query range from 1.5ms to 5.7s</li>
  <li>Computational requirements range from 0.48 to 24 GFLOPS</li>
  <li>The hardware cost of hardware ranges m $0.096/hr for 2 vCPUs to $3.06/hr for a V100 GPU</li>
</ul>

<p><code>Problem1: How to dynamically choose the right model variant among 4032 search spaces</code>?</p>

<h3 id="scaling">Scaling</h3>

<p><code>Static provisioning</code> (Tensorflow Serving) is based on peak load, it wastes resources at low load.  =&gt; leading to high cost.</p>

<p><code>Replica-only</code> suffers from</p>

<ul>
  <li>High start-up latency and the right variant may change with load.</li>
  <li>Replication statically-fixed cannot meet the new requirement</li>
  <li>Replication may have a large delay since new resource may not available,</li>
</ul>

<p><code>Problem2: How to scale to meet the load requirements while minimizing the cost?</code></p>

<h3 id="resource-utilization">Resource utilization.</h3>

<p>When the load is low, idle model taking resources is a waste.</p>

<p><code>Problem3: how to improve resource utilization?</code></p>

<h2 id="contribution">Contribution</h2>

<p>The paper proposes INFaaS -  an automated model-less system for distributed inference serving. it can</p>

<ol>
  <li>
    <p>Selects the best model-invariant based on user-specified requirements like cost, accuracy, latency.</p>
  </li>
  <li>
    <p>Support <code>model-horizontal scaling, model-vertical scaling, and VM-autoscaling.</code></p>

    <p>It can dynamically react to changing applications and request patterns <code>such that the total cost is the lowest</code>. eg, multiple model-variants to serve one type of request instead of replicating one model-variant many times.</p>
  </li>
</ol>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220302161845052.png" alt="image-20220302161845052" /></p>

<ol>
  <li>
    <p>Support multi-tenancy by sharing resources across applications and models when the workload is low. When the load is low, the performance is almost the same.</p>

    <p>The point when co-location starts affecting the performance varies across models and depends on both the load and the hardware architecture.</p>
  </li>
</ol>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220302162948681.png" alt="image-20220302162948681" /></p>

<h1 id="infaas">INFaaS</h1>

<p><strong>Design principles</strong></p>

<ul>
  <li>Declarative API: developer only focuses on high-level <code>latency, cost, or accuracy</code> requirements.</li>
  <li>INFaaS should automatically and efficiently select a model-variant for</li>
  <li>Serve each query</li>
  <li>Scale in reaction to changing application load (when the load is increasing?)</li>
  <li>Improve resource utilization.  resource sharing</li>
  <li>the system is modular and extensible.</li>
</ul>

<h2 id="users-interface">Userâ€™s interface</h2>

<p><strong>Model Registration</strong></p>

<p>User register trained-models ( ONNX Format ) with a validation-dataset (eg,. ResNet50) and assigns AppID, ModelID, ModelName. One application can include many models for different subtask</p>

<p>Backend test accuracy using validation-dataset</p>

<p><strong>Query submission</strong></p>

<p>User submit query with latency, cost, accuracy (eg, . Latency = 200ms,  accuracy &gt;=70%). And then the backend searches model variants and <code>scaling strategies.</code></p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220302170229107.png" alt="image-20220302170229107" /></p>

<h2 id="architecture">Architecture</h2>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220302194903445.png" alt="image-20220302194903445" /></p>

<h3 id="model-variant-selection-policy">Model-variant selection policy</h3>

<p>Triggered when</p>

<ol>
  <li>
    <p>The arrival of a new query.</p>

    <p>The controllerâ€™s dispatcher will use the policy to select a variant.</p>
  </li>
  <li>
    <p>On changes in query load.</p>

    <p>The worker will monitor the incomming query load and curernt throughput.</p>

    <p>If a change is detected, it will use the policy in the background to determine <code>vertically scale or horizontally scale.</code></p>
  </li>
</ol>

<h3 id="metadata-store">Metadata Store</h3>

<p>Store:</p>

<ol>
  <li>stores modelâ€™s profile (accuracy, latency), resource usage, load statistics</li>
  <li>worker machines.</li>
</ol>

<p>O(1) get complexity.</p>

<h3 id="model-repository">Model repository</h3>

<p>persistent storage, store serialized model.</p>

<h1 id="selecting-and-scaling-model-variants">Selecting and Scaling Model Variants</h1>

<p>Model selection policy consider both</p>

<ol>
  <li><code>static states (model's profiles etc)</code></li>
  <li><code>dynamic states</code>: variant may in following states:
    <ul>
      <li>Have not be loaded, mainly consider loading latency (<code>Inactive</code>)</li>
      <li>Loaded but serving at its peaking throughput. (<code>Overloaded</code>)</li>
      <li>Loaded but experiencing resource contention. (<code>Interfered</code>)</li>
      <li>Have not be loaded due to lack of resources required. (<code>Interfered</code>)</li>
    </ul>
  </li>
</ol>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220302202324207.png" alt="image-20220302202324207" /></p>

<p>Dynamic states are stored at meteStore and maintain by workerâ€™s monitoring daemons.</p>

<h2 id="on-arrival-of-query">On arrival of query</h2>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220302203846843.png" alt="image-20220302203846843" /></p>

<p><strong>Mitigating variants</strong></p>

<p>The system mitigate <code>interfered</code> variants to</p>

<ul>
  <li>firstly check available resource in <code>same worker</code> in background thread.</li>
  <li>If not available, then ask controller dispatcher to dispatch the variant on the least loaded worker.</li>
</ul>

<p>The model auto scaler assesses whether scale <code>overloaded</code> variants to different variant.</p>

<h2 id="on-changes-in-query-load">On changes in query load</h2>

<h3 id="model-autoscaler-at-each-worker">Model-Autoscaler at each worker</h3>

<h1 id="implementation">Implementation</h1>

<h1 id="evaluation">Evaluation</h1>

:ET