I"ë<p>DARTS: DIFFERENTIABLE ARCHITECTURE SEARCH</p>

<h1 id="introduction">Introduction</h1>

<h2 id="current-problems">Current problems</h2>

<p>The architecture search algorithm is computationally demanding.</p>

<ul>
  <li>RL for NAS needs 2000 GPU days.</li>
  <li>Even with optimizations like weight prediction, performance prediction,  weight sharing,</li>
  <li>The main reason for this is all searching methods (RL, Bo, etc) treated NAS as a block box optimization over a <strong>discrete</strong> domain</li>
</ul>

<h2 id="contributions">Contributions</h2>

<p>It treats NAS from different angles. Instead of searching over a discrete set of candidate architectures, we <strong>relax the search space to be continuous,</strong> so that the architecture can be optimized with respect to its validation set performance by gradient descent.</p>

<ol>
  <li>introduce a new algorithm for differentiable NAS based on bilevel optimization.</li>
  <li>Improve the efficiency (days)</li>
  <li>good transferable, trained with CIFAR-10 has good performance at ImageNet.</li>
</ol>

<h1 id="differentiable-architecture-search">Differentiable Architecture Search</h1>

<h2 id="search-space">Search Space</h2>

<p>Search computation cells as building blocks of final architecture.</p>

<p>Each cell is a DAG graph consisting of many nodes. Each node is a matrix/tensor. Each edge is associated with some operations.</p>

<p><code>Assume</code> each cell has two input nodes and a single output node.</p>

<ol>
  <li>The output of the cell is obtained by applying concatenation to all intermediate nodes.</li>
  <li>The intermediate node is computed with all predecessors.</li>
</ol>

<h2 id="continuous-relaxation-optimization">CONTINUOUS RELAXATION Optimization</h2>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220427221154302.png" alt="image-20220427221154302" /></p>

<p>The final problem is defined above</p>

<h2 id="approximate-solution">Approximate Solution</h2>

<p>Run gradient decent together, each iteration update alpha and w together.</p>

<p>The step1 and step2 are updating together, use alpha and w from previous step.</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220427221114843.png" alt="image-20220427221114843" /></p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220427221950638.png" alt="image-20220427221950638" /></p>

<h2 id="deriving-discrete-architecture">Deriving discrete architecture</h2>

<p>To form each node in the discrete architecture, we retain the top-k strongest operations (from distinct nodes) among all non-zero candidate operations collected from all the previous nodes. Strength is defined as softmax as shown above.</p>

<h1 id="experiments-and-result">Experiments and Result</h1>

<h2 id="architecture-search">Architecture search</h2>

<h3 id="convolutional-cells-for-cifar-10">Convolutional cells for CIFAR-10</h3>

<p><strong>Each cell has 7 nodes. The first and second nodes of cell k are set equal to the outputs of cell k-2 and cell k-1, respectively,</strong></p>

<p>Operations between nodes:</p>

<ul>
  <li>3X3 and 5X5 separable convolutions</li>
  <li>3X3 and 5X5 dilated separable convolutions</li>
  <li>3X3 max pooling</li>
  <li>3X3 average pooling</li>
  <li>zero.</li>
</ul>

<p>â€‹	Where it uses ReLU-Conv-BN order for convolutional operations</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220427222735713.png" alt="image-20220427222735713" /></p>

<h2 id="architecture-evaluation">ARCHITECTURE EVALUATION</h2>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220427222829736.png" alt="image-20220427222829736" /></p>

:ET