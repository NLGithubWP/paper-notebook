I",<h1 id="introduction">Introduction</h1>

<h2 id="current-problems">Current problems</h2>

<ol>
  <li>Hard to design a new network, and its computation is expensive.</li>
  <li>search spaces design has a bias toward human expertise. So the final model is sub-optimal.</li>
</ol>

<h2 id="contribution">Contribution</h2>

<p>The main contribution of this paper is to propose a novel approach to search for DNN architectures aimed at resolving the two search problems mentioned above by</p>

<ol>
  <li>
    <p>defining an incremental search; (tower, blocks. )</p>
  </li>
  <li>
    <p>using a transferable training;   (weight sharing)</p>
  </li>
  <li>
    <p>using a set of generic neural network blocks (<strong>recurrent or a convolutional layer)</strong></p>
  </li>
  <li>
    <p><strong>Using ensembling to reduce the parameters of a single architecture.</strong></p>

    <p>Ensembling is a well-studied field it is domain agnostic and it is a natural way to increase the size of the network given a good performing model architecture.</p>
  </li>
</ol>

<p><strong>The searched architecture has fewer parameters and higher performance.</strong></p>

<h1 id="algorithm">Algorithm</h1>

<p>The algorithm first defines the architecture as a combination of k blocks. Each block has n options.</p>

<p>And then search for the best architecture over all possible candidates.</p>

<ol>
  <li>The search run in distributed asynchronous fashion. (each trainer will run search algorithm 1 ) after searching, it will record it to A.</li>
  <li>The search will do a mutation, which will explore the depth and then do the exploitation.</li>
  <li>After finding the best architecture, it uses algorithm3 to produce an average weighted ensemble of a number of repetitions of that candidate, retraining them from scratch with the different shuffling of the data and different initialization parameters.</li>
</ol>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220511225217874.png" alt="image-20220511225217874" /></p>

<h1 id="result">Result</h1>

<h2 id="setting">Setting</h2>

<p>Search phase use 10 million steps</p>

<p>Ensembling is invoked using 50 million steps.</p>

<p>15 trainers, each with 350 workers. Searching for one week.</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220511231057286.png" alt="image-20220511231057286" /></p>

<h2 id="result-1">Result</h2>

<ol>
  <li>
    <p>When ensembling this configuration twice (p = 2), we improve the accuracy from 59% to 62.77%.</p>
  </li>
  <li>
    <p>Compare the accuracy of searched architecture and human-designed architecture.</p>

    <p>The searched architectures are smaller and converge faster than the existing ones</p>
  </li>
  <li>
    <p>Show the search architectureâ€™s blocks.</p>
  </li>
  <li>
    <p>performace / steps.</p>
  </li>
</ol>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220511230852626.png" alt="image-20220511230852626" /></p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220511231130684.png" alt="image-20220511231130684" /></p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220511231210201.png" alt="image-20220511231210201" /></p>

<p>###</p>
:ET