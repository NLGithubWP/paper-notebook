I"t3<p>Retiarii: A Deep Learning Exploratory-Training Framework</p>

<h1 id="current-problems">Current Problems</h1>

<p>There are three ways to create a new model candidate.</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220419161111915.png" alt="image-20220419161111915" /></p>

<p>The exploratory-training process is not supported well in current software.</p>

<ol>
  <li>
    <p><strong>Search space</strong>:  In the current system, like PyTorch/tf, we have to code up all variations of models in one jumbo model, and there is a control flow to pick one during model construction. <strong>The control flow in this Jumbo model</strong> makes it hard to optimize memory usage, operator fusion, etc.</p>

    <p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220419165437608.png" alt="image-20220419165437608" /></p>
  </li>
  <li>
    <p><strong>Search Strategy</strong>: Exploration strategy is responsible for</p>

    <ul>
      <li>deciding which models to instantiate and train. in which priority, and when to terminate. eg, Random Search, Grid Search, Heuristic-based search, Bayesian-based. and reinforcement learning.</li>
      <li>Manage execution of training, eg. stop training the bad-performance model, and add more resources to model with good performance. Share weights of overlapped layers.</li>
    </ul>

    <p>The implementation of a Search Strategy often tightly couples an exploration strategy with a specific model space, which includes 2 problems:</p>

    <ul>
      <li><strong>poor reusable</strong>: it’s hard to reuse a search strategy designed for one search space by another search space.</li>
      <li><strong>hard to scale</strong>: It’s hard to cross-model training or distributed training with Multiple GPU.</li>
    </ul>
  </li>
</ol>

<h1 id="contribution">Contribution</h1>

<p>The paper’s system clearly decouples model space from exploration strategy and enables system optimizations to speed up the exploration process.</p>

<ol>
  <li>
    <p>New programming interface ( Mutator abstraction ) to</p>

    <ul>
      <li>specify DNN model space for exploration.</li>
      <li>specify exploration strategy to decide
        <ul>
          <li>order to instantiate and train the model.</li>
          <li>prioritize model training.</li>
          <li>when to terminate training.</li>
        </ul>
      </li>
    </ul>

    <p>The Retiarii use Mutator abstraction for the above specification ( Search space and Strategy ).</p>

    <ul>
      <li>The Search Space = <code>A set of base models and mutators.</code></li>
      <li>The Search Strategy = which base model and a mutator to use +  <code>when</code> to apply mutators to the base model.</li>
    </ul>

    <p>Each mutator is fine-grained, and captures a logical unit of modification. Reusable and compostable</p>
  </li>
  <li>
    <p>Offers just-in-time engine to instantiate model, manage the training of the instantiated model, gather information of exploration strategy to consume, execute the decision.</p>
  </li>
  <li>
    <p>Offers <code>cross-model optimizations</code> to improve the overall exploratory training process by using correlation information.</p>
  </li>
</ol>

<h2 id="evaluation-result">Evaluation result</h2>

<ol>
  <li>reduce the exploration time of popular NAS algorithm by 2.57X</li>
  <li>Improve scalability of NAS using weight sharing with a speed-up of 8.58 X</li>
</ol>

<h1 id="mutator-as-the-core-abstraction">Mutator as the Core Abstraction</h1>

<ol>
  <li>
    <p>Rather than encoding modification in the complex jumbo model, the system import the existing model from Tf/Pytorch as a base model and apply a mutator to generate a new model.</p>

    <p>There are three Mutator in the system</p>

    <ul>
      <li>Input Mutator: mutate inputs of matched operators</li>
      <li>Operator mutator: replace matched operator with other operators</li>
      <li>Insert mutator: Insert new operators or sub-graphs</li>
    </ul>

    <p>The search space includes all base models and new models.</p>
  </li>
  <li>
    <p>The system can record the relation between models generated by different mutators based on the same base model. eg, for two instantiations of the same base model, the nodes not modified by mutator are considered identical.</p>

    <p>The relation can be used to optimize the multi-model training.</p>
  </li>
  <li>
    <p>The mutator can be applied to any subgraph of the model. And create a new model instance.</p>

    <p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220419175540302.png" alt="image-20220419175540302" /></p>
  </li>
</ol>

<h1 id="retiarii-just-in-time-engine">Retiarii Just-In-Time Engine</h1>

<p>The engine will instantiate the model on the Floy and manage the training of the model dynamically.</p>

<p><code>Inputs</code>: based models, mutators, policy describing exploration strategy.</p>

<p><code>Execution</code>: Pick one base model and a mutator to generate a new model using strategy.</p>

<p>Strategy can be</p>

<ul>
  <li>context-free strategy: random choice</li>
  <li>history-based strategy. etc</li>
  <li>customization choice.</li>
</ul>

<p>The engine records the mutation history, thus it knows which nodes are not modified and stay identical. So the engine can perform corse-model optimization like</p>

<ul>
  <li>common sub-expression elimination,</li>
  <li>cross model operator batching</li>
  <li>NAS optimization</li>
</ul>

<p>The optimized Data flow graph is then converted to a standard model format for the existing DL framework to perform single-model optimization before training.</p>

<p>The strategy also responsible for</p>

<ul>
  <li>Launch training on the new model,</li>
  <li>monitor the training and collect results,</li>
  <li>adjust training resource allocation</li>
  <li>Terminate training of less promising models.</li>
</ul>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220419182139203.png" alt="image-20220419182139203" /></p>

<h1 id="cross-model-optimization">Cross-Model Optimization</h1>

<h2 id="optimization-opportunities">Optimization Opportunities</h2>

<h3 id="common-sub-expression-elimination">Common sub-expression elimination</h3>

<p>Compute the identical operations only once.</p>

<p>It can be applied to non-trainable operations such as data loading and preprocessing. since it’s determinstic operation. while in training, weight is changing.</p>

<p><strong>Evaluation</strong></p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220419213808406.png" alt="image-20220419213808406" /></p>

<h3 id="operator-batching">Operator Batching</h3>

<p>Common operators with different inputs and weights can potentially be batched together and computed in a single operator kernel.</p>

<ol>
  <li>Two graphs that share multiple layers with the <strong>same weights</strong> can be merged. As shown below.</li>
  <li>Two operations with different weights can also be batched with special kernels like <strong>grouped convolution, and batch_matmul</strong>. That can parallel compute on slices of an input tensor.</li>
</ol>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220419230838890.png" alt="image-20220419230838890" /></p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220419184057269.png" alt="image-20220419184057269" /></p>

<p>group convolution:</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220419232624659.png" alt="image-20220419232624659" /></p>

<h3 id="weight-sharing">Weight sharing</h3>

<p>Instead of training the graph’s weight from scratch, shared weights are inherited from other graphs to continue the training in this graph. And only the different nodes will have different weights.</p>

<ol>
  <li>It can let user developers to annotate operator weights they want to share.</li>
  <li>it will identify the weight-sharing-enabled operators in common subgraphs.</li>
</ol>

<p>The system incure a new type of parallelism when constructing executable graphs.</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220419225513233.png" alt="image-20220419225513233" /></p>

<p>The System builds a super-graph automatically, And we don’t need to store the check-point on disk and then reload.</p>

<p><strong>Evaluation</strong></p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220419225626822.png" alt="image-20220419225626822" /></p>

<h2 id="executable-graph-construction">Executable Graph Construction</h2>

<p>To exploit the above optimization, the system needs to construct graphs from raw models.</p>

<p>The construction involves:</p>

<ul>
  <li>Model merging,</li>
  <li>device placement of operators.</li>
  <li>Training parallelism</li>
</ul>

<h3 id="device-placement">Device placement</h3>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220419190028675.png" alt="image-20220419190028675" /></p>

<p>For DFGs sharing the same dataset and preprocessing, these common operators can be merged by common sub-expression elimination.</p>

<p>The system will test each model for a few iterations and then sort them based on iteration time.  The system will then pack as many as models possible.</p>

<p><strong>Evaluation</strong></p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220419214307634.png" alt="image-20220419214307634" /></p>

<h3 id="mixed-parallelism-for-weight-sharing">Mixed parallelism for weight sharing.</h3>

<p>The system uses both data parallelism and model parallelism to train the network.</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220419191852896.png" alt="image-20220419191852896" /></p>

<p><strong>Evaluation</strong></p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220419222635225.png" alt="image-20220419222635225" /></p>

<h1 id="evaluation">Evaluation</h1>

<h2 id="main-founding">Main founding</h2>

<ol>
  <li>
    <p>The separation of model space and exploration strategy makes it easy for Retiarii to try different combinations.  Retiarii currently supports 27 popular Neural Architecture Search (NAS) solutions. Most of them can be implemented by the three mutator classes provided by Retiarii.</p>
  </li>
  <li>
    <p>A number of micro-benchmarks show how Retiarii’s cross model optimizations greatly improve training efficiency.</p>
  </li>
  <li>
    <p>Retiarii improves the model exploration speed of three NAS solutions by up to 2.58°ø, compared with traditional approaches.</p>
  </li>
  <li>
    <p>Retiarii improves the scalability of weight sharing-based NAS solutions and brings up to 8.58°ø speed-up using the proposed mixed parallelism, compared with data parallelism.</p>
  </li>
</ol>

<h2 id="micro-benchmarks">Micro benchmarks</h2>

<h3 id="shard-data-loading-and-preprocessing">Shard data loading and preprocessing</h3>

<p>We compare Retiarii with a baseline that runs each model independently without common sub-expression elimination.</p>

<h3 id="operator-batching-1">Operator batching</h3>

<p>Insert an adapter layer to a pre-trained Mobile Net, and multiple mobile Net shares the same weights, only the adaptor is different.</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220419184057269.png" alt="image-20220419184057269" /></p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220419214446724.png" alt="image-20220419214446724" /></p>

<p>Overall, Retiarii’s operator batching improves the aggregate throughput by 3.08°ø when batching 192 models, compared with the baseline that can only train at most 12 models together. Retiarii can batch more models than the baseline because it only has one copy of (fixed) weights from MobileNet. Only the memory for adapters is increased when batching more models</p>

<h3 id="weight-sharing-1">Weight sharing</h3>

<p>Compare three cases</p>

<ol>
  <li>weight is saved and loaded through files</li>
  <li>weight is saved and loaded through the object in memory</li>
  <li>the system’s super graph with cross-modal optimization.</li>
</ol>

<h2 id="speeding-up-nas">Speeding up NAS</h2>

<p>Using MnasNet, NASNet, AmoebaNet.</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220419225009849.png" alt="image-20220419225009849" /></p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220419222323121.png" alt="image-20220419222323121" /></p>

<ol>
  <li>Retiarii is substantially faster than the two baselines due to the cross-model optimizations</li>
</ol>

<h2 id="scale-weight-shared-training">Scale weight shared training</h2>

:ET