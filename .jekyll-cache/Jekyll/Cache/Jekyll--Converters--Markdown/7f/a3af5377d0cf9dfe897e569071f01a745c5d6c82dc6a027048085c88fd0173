I"G<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220509214808794.png" alt="image-20220509214808794" /></p>

<h1 id="introduction">Introduction</h1>

<h2 id="motivation">Motivation</h2>

<ol>
  <li>
    <p>GNN cannot scale well to data size and message passing steps. The exponential growth of neighborhood size leads to exponential IO overhead (a major challenge in large-scale GNN.)</p>
  </li>
  <li>
    <p>Some work tries to train GNN in a distributed way, but the aggregation procedure bottlenecked the speed.</p>

    <p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220510171759320.png" alt="image-20220510171759320" /></p>
  </li>
  <li>
    <p>There is no general design space for GNN. And the exploration of search space is extensive.</p>
  </li>
</ol>

<h2 id="contribution">Contribution</h2>

<p>The paper proposes the first paradigm and system</p>

<ol>
  <li>
    <p>Introduce scalable graph neural architecture paradigm with some abstractions</p>

    <ul>
      <li><strong>graph_aggregator</strong>: captures the structural information via graph aggregation operations.</li>
      <li><strong>message_aggregator</strong> combines different levels of structural information.</li>
      <li><strong>message_updater</strong> generates the prediction based on the multi-scale features.</li>
    </ul>

    <p>With those abstractions, the system can define general design space, and decouple sampling and training.</p>
  </li>
  <li>
    <p>Propose a general design space consisting of 6 design dimensions, including 150k possible designs of scalable GNN.</p>

    <p>And the space has adaptive aggregation and a complementary post-processing stage</p>
  </li>
  <li>
    <p>Propose a search system to search a GNN.</p>

    <ul>
      <li>Suggestion engine (multi-objective search algorithm)</li>
      <li>Evaluation engine in a distributed manner</li>
    </ul>
  </li>
</ol>

<h1 id="abstraction">Abstraction</h1>

<p>The paper divides the GNN training process into 3 stages. And each stage has many optional operations, which define the overall search space.</p>

<p>Many existing GNN models can be generalized from the defined search space.</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220510215212171.png" alt="image-20220510215212171" /></p>

<h1 id="engines">Engines</h1>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220511114307524.png" alt="image-20220511114307524" /></p>

<h1 id="experiments">Experiments</h1>

<h2 id="setting">Setting</h2>

<p>Datasets:</p>

<ul>
  <li>citation networks (Citeseer, Cora, and PubMed)</li>
  <li>two social networks (Flickr and Reddit),</li>
  <li>co-authorship graphs (Amazon and Coauthor)</li>
  <li>co-purchasing network (ogbn-products)</li>
  <li>one short-form video recommendation graph (Industry)</li>
</ul>

<p>Baselines: compare with GCN, GAT, JK-Net, Res-GCN, APPNP, AP-GCN, SGC, SIGN, S2GC and GBP</p>

<h2 id="searched-representatives">Searched Representatives</h2>

<p>We apply the multi-objective optimization targeting at classification error and inference time on Cora.</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220510222846980.png" alt="image-20220510222846980" /></p>

<h2 id="training-scalability">Training scalability</h2>

<p>choose PaSca-APPNP as a representative and compare it with GraphSAGE</p>

<p>Train both of them with</p>

<ol>
  <li>batch size is 8192 for Reddit and 16384 for ogbn-product</li>
  <li>in stand-alone and distributed scenarios and then measure their corresponding speedups.</li>
  <li>speedup is calculated by runtime per epoch ( one worker in the stand-alone scenario and two workers in the distributed scenario )</li>
  <li>WIthout cost, expectation is linear increase. ( since it’s async dist train )</li>
</ol>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220510223238526.png" alt="image-20220510223238526" /></p>

<p>GraphSage requires aggregating the ndoes during training, and it meets I/O bottleneck.</p>

<h2 id="performance-efficiency-analysis">Performance-Efficiency Analysis</h2>

<p>PaSca-V3 achieves the best performance with 4°ø training time compared with GBP and PaSca-V1. Note that, though PaSca-V1 requires the same training time as GBP, its inference time is less than GBP</p>

<p>So we can choose PaSca-V1 to V3, along with GBP, according to different requirements of predictive performance, training efficiency, and inference time.</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220510224228052.png" alt="image-20220510224228052" /></p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220510224153663.png" alt="image-20220510224153663" /></p>

<h2 id="model-scability">Model Scability</h2>

<p>It includes adaptive message_aggregator and the adaptive message_aggregator can identify the different message-passing demands of nodes and explicitly weight each graph message.</p>

:ET