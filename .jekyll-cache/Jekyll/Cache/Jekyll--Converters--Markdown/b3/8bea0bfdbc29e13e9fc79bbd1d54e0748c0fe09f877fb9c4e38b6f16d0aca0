I"‚<h1 id="introduction">Introduction</h1>

<h2 id="motivation">Motivation</h2>

<p>In serverless computing, the unit of computation is a function. When a service request is received, the serverless platform allocates an <strong>ephemeral execution environment</strong> for the associated function to handle the request.</p>

<p>For more complex applications, an Action sequence is used to connect multiple functions into a single service.</p>

<p>Overall, serverless can separate the function into modules, each function should start up as quickly as possible.</p>

<p>But there are high start-up delays for function execution and inefficient resource usage.</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220613184210071.png" alt="image-20220613184210071" /></p>

<p>There are two reasons</p>

<ol>
  <li>Each application is executed in a separate container instance.
    <ul>
      <li>Cold-start: High start-up latency, it starts a container, and installs libraries ( 80% taken). ( few seconds in AWS Lambda. )</li>
      <li>Warm-start:
        <ul>
          <li>It uses a pre-warming technique to speed up the first call, but it occupies resources for the idle period.</li>
          <li>It relaxes the original isolation guarantee, since many requests may be handled inside the same container.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Internal function call also passes from the <strong>controller</strong>, which incurs extra latency.</li>
</ol>

<h2 id="contribution">Contribution</h2>

<p>The paper presents a system to provide:</p>

<ol>
  <li>Lower latency: Function runs as a process inside the same container.  (fast allocate/delete)</li>
  <li>High resource efficiency: Process takes less memory compared with a new container.</li>
  <li>More elasticity: Shared message bus at each host for function communication. (fast triggering of functions running on the same host)</li>
</ol>

<p>The idea is</p>

<ol>
  <li>Two levels of isolation to enable <strong>quickly allocate/release of resources</strong>: between applications / between functions of the same application.</li>
  <li>Use hierarchical <strong>message</strong> queuing and <strong>storage</strong> mechanism to <strong>leverage locality for interacting functions</strong> of the same application.
    <ul>
      <li>Functions in the same application are orchestrated locally.</li>
      <li>Use a local message bus on each host, such that functions executing in a sequence can start instantly.</li>
    </ul>
  </li>
</ol>

<p>Experiments show it achieves 43% speed-up compared with OpenWhisk.</p>

<h2 id="limitations">Limitations</h2>

<ol>
  <li>Functions run in the same container at the same host may compete for the same resources and interfere with each otherâ€™s performance.
And the scheduling policy could lead to sub-optimal load balancing.</li>
  <li>In this system, the CPU time could be used for billing purposes. But the process contention could increase the latency of the application. And the function cannot get a fair share of resources.</li>
  <li>The system use fork to run a new function, so it cannot support language runtime without native forking.</li>
</ol>

<h1 id="background">Background</h1>

<h2 id="functions-in-serverless">Functions in serverless</h2>

<p><strong>Deploy</strong>: Functions are <strong>mapped</strong> into containers to achieve the goal.</p>

<p><strong>Call</strong>: When the cloud receives usersâ€™ requests, it can cold start, or warm start to launch a function ( with container ) to run usersâ€™ requests.</p>

<p><strong>Concurrency</strong>: Cloud providers allow <strong>only one execution</strong> at a time in a container for performance isolation ( run a new container in cold start, wait in a queue in warm start ). While OpenFaaS allow concurrent executions of the <strong>same</strong> function in a <strong>single</strong> container.</p>

<p><strong>Chaining</strong>: Events that trigger function can be external or internal, Most existing systems treat those events the same.</p>

<h1 id="system-design">System Design</h1>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220613211120475.png" alt="image-20220613211120475" /></p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220613211135593.png" alt="image-20220613211135593" /></p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220613211147247.png" alt="image-20220613211147247" /></p>

<h1 id="evaluation">Evaluation</h1>

<p>The paper evaluates SAND with OpenWhisk and AWS Greengrass.</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220613212359956.png" alt="image-20220613212359956" /></p>

<h2 id="start-latency">Start latency</h2>

<p>spawning processes with binaries (exec C, exec Go) are faster than interpreted languages (exec Python, exec NodeJS) and Java, and forking processes (fork C, fork Python) is fastest among allã€‚</p>

<h2 id="hierarchical-message-pass">Hierarchical Message pass</h2>

<p>Local message bus on every host for fast function interaction, local message bus 2.90 faster than via the global message bus.</p>

<h2 id="function-interaction-latency">Function interaction latency</h2>

<p>Measure the time between the first functionâ€™s finish and the second functionâ€™s start. All functions communicate with the local hostâ€™s shared memory.  faster 9 ms.</p>

<h2 id="memory-usage">Memory Usage</h2>

<p>50 concurrent calls to a single Python function.</p>

<p>Each call adds to the memory footprint of about 14.61MB and 13.96MB in OpenWhisk and Greengrass, respectively.</p>

<p>In SAND, each call only adds 1.1MB on top of the 16.35MB consumed by the grain worker.</p>

<p>This difference is because SAND forks a new process inside the same sandbox for each function call, whereas OpenWhisk and Greengrass use separate containers for concurrent calls.</p>

<h2 id="idle-memory-cost-vs-latency">Idle Memory Cost vs latency</h2>

<p>The container will keep warm for a while if there are requests that come until timeout.</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220613221335876.png" alt="image-20220613221335876" /></p>

<h2 id="cases">Cases</h2>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220613220624696.png" alt="image-20220613220624696" /></p>

:ET