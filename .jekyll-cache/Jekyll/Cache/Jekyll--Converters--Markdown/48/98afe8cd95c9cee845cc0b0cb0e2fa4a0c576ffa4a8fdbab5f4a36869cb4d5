I"I<h1 id="introduction">Introduction</h1>

<h2 id="motivation">Motivation</h2>

<p>It’s important to <strong>automatically scale</strong> the model selection on a cluster to <strong>increase throughput</strong> <strong>without raising resource costs</strong>. The paper proposes a system to meet the above requirements.  Specifically, the system design has the following consideration.</p>

<ol>
  <li>Scalability:  model selection scalability has a positive correlation to <strong>task scalability or data scalability.</strong></li>
  <li>High throughput: how many configurations are evaluated per unit time.</li>
  <li>Resource efficiency:
    <ul>
      <li>Per-epoch efficiency: time to complete an epoch.</li>
      <li>Convergence efficiency: time to complete training.</li>
      <li>Memory/storage efficiency: memory/disk usage.</li>
      <li>Communication efficiency: network bandwidth usage.</li>
    </ul>
  </li>
  <li>Reproducibility</li>
</ol>

<p>However, the existing model selection system doesn’t achieve the above aspects well.</p>

<ol>
  <li>They have not increased overall scalability by sharding the data ( data scalability )</li>
  <li>Bring data scalability into the system without wasting memory, and network bandwidth in the model selection computing process is not easy. Decouple computing with storage =&gt; high network bandwidth or potential cache.</li>
</ol>

<h2 id="contribution">Contribution</h2>

<p>The paper builds a model selection system with a well-designed combination of task-parallelism and data-parallelism. it can</p>

<ol>
  <li>Increase model-selection throughput. (3-10X)</li>
  <li>Save memory/storage usage. (8x)</li>
  <li>Save network communications (100X)</li>
</ol>

<h1 id="system-design">System Design</h1>

<h2 id="parallelism-summary">Parallelism summary.</h2>

<p>Parallelism is a popular way to raise throughput.</p>

<ol>
  <li>
    <p>Task Parallelism requires copying data into each worker, and there is no communication between them =&gt; high storage cost.</p>
  </li>
  <li>BSP (Bulk Synchronous Parallel): master average the weights / gradients per epoch =&gt; poor converge.</li>
  <li>Sync ps / Asyn ps =&gt; high communication overhead.</li>
  <li>All reduce =&gt; high communication overhead.</li>
</ol>

<h2 id="system">System</h2>

<p>The system is basically composed of two parts.</p>

<ol>
  <li>scheduling: Try to achieve the best scheduling policy, graph D</li>
  <li>failure recovery:  Cerebro detects failures via the periodic heart-beat check between the scheduler and workers.</li>
</ol>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220627212504654.png" alt="image-20220627212504654" /></p>

<h1 id="evaluation">Evaluation</h1>

<p>Workloads: Two neural architectures and hyperparameters =&gt; 16 cfgs.</p>

<p>Compare with Horovod,</p>

<p>GPU cluster with 8 workers and 1 master.</p>

<h2 id="throughput--efficiency">Throughput &amp; Efficiency</h2>

<p>The paper shows the system is fast and resource-efficient.</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220627213706340.png" alt="image-20220627213706340" /></p>

<h2 id="scalability">Scalability</h2>

<p>linear speedups due to MOP’s marginal communication costs.</p>

<p>In contrast, Horovod exhibits substantially sub-linear speedups due to its much higher communication costs with multiple workers</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220627214421155.png" alt="image-20220627214421155" /></p>

<h2 id="effect-of-batch-size">Effect of batch size</h2>

<p>When batch size increases, the runtime of Cerebro is reduced because larger batch sizes fully use the hardware computing capacity.</p>

<p>A larger batch size also reduces the runtime of Horovod because the communication is less.</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220627220559013.png" alt="image-20220627220559013" /></p>

<h1 id="related-system">Related system</h1>

<p>Google Vizier, Ray Tune, Dask-Hyperband, SparkDL, and Spark-Hyperopt.</p>

:ET