I"=<p>Some questions about the paper.</p>

<ol>
  <li>Why the logs are in cache instead of the main memory? how could they control this?</li>
</ol>

<h1 id="abstract--introduction">Abstract &amp; introduction</h1>

<h2 id="background">Background</h2>

<p>It is desirable to replciate the microsecond apps  across many hosts to provide high availability. Especially for the <strong>stateful services like kv store.</strong></p>

<p>Traditional State Machine Replication system add hundreds of microseconds of overhead on replication process.</p>

<p>Recend work explores modern hardware to decrease the replication time to several microseconds. Eg Hermes, Dare, HovercRaft. But it’s still to high for apps taking only few microsecond.</p>

<p>When above meet failure, it spends tens of milliseconds to recover.</p>

<h2 id="problems">Problems</h2>

<p>How to make microsecond apps fault-tolerant through replication which operates at microsecond scale?</p>

<h2 id="contribution">Contribution</h2>

<p>The pape propose a system called MU,  it leverages RDMA to implement strong consistency for microsecond apps.</p>

<p>Summary of contributations</p>

<ol>
  <li>
    <p>It significantly <strong>reduce both replication and failover recover latency.</strong> 
Specifically, it spends less than 1.3 microseconds to replicate a app request and 873 microseconds to failure recover.</p>

    <p>Roughly, leader replicates a request by simple writing it directly to the log of other replicas using RDMA in <strong>parallel way and in one round</strong>, without any additional communication.</p>
  </li>
  <li>
    <p>Provides complete correctness proof of Mu</p>
  </li>
  <li>
    <p>Evaluate its raw performance</p>
  </li>
</ol>

<p>Callenge of this system:</p>

<ol>
  <li>Mechanism to protect against reces of concurrent leaders.</li>
  <li>Change leaders and garbage collect logs, as described in the paper</li>
</ol>

<p>Experiments: The system is evaluated on a financial exchange app, Redis, Memcached, HERD.</p>

<h1 id="backgrounds">Backgrounds</h1>

<h2 id="microsecond-services">Microsecond services</h2>

<p>Modern distributed systems are composed of hundreds of stateless and stateful microservices, such as key-value stores, web servers, load balancers, and ad services—each operating as an independent app whose latency requirements are gradually decreasing to the microsecond level as the number of composed services is increasing</p>

<h2 id="state-machine-replication">State machine replication</h2>

<p>A common way to implement SMR is as follows: each replica has a copy of the service software and a log. The log stores client requests.</p>

<p><strong>A consensus protocol must ensure <em>safety</em> and <em>liveness</em></strong></p>

<h2 id="rdma">RDMA</h2>

<p>Traditional communication process:</p>

<ol>
  <li>in sender party: data is copied from user space to kernel socker buffer</li>
  <li>in sender party: data is added with tcp/ip headers</li>
  <li>in sender party: data is copied from socker buffer to NIC buffer.</li>
  <li>in receiver party: data is copied from NIC buffer to socker buffer</li>
  <li>in receiver party: data is read with header information</li>
  <li>in receiver party: data is copied from socker buffer to user space buffer.</li>
  <li>Process trigger context switch.</li>
</ol>

<p>RDMA is based on Kernel bypass and zero copy technology.</p>

<ol>
  <li>CPU Offload: No need CPU. Process can access remote host’s memory without bother either sender’s or receiver;s CPU.</li>
  <li>Kernel Bypass: RDMA provides a Verbs interface instead of traditional TCP/IP socket interface. User’s process can communicate with other host without system call (no context switch. )</li>
  <li>Zero Copy:  User’s process can communicate with RNIC hardware directly.</li>
</ol>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220108215101404.png" alt="image-20220108215101404" /></p>

<p>Programming model:</p>

<p>Applications make <strong>local memory available for remote access</strong> by registering <strong>local virtual memory regions (MRs) with the RDMA driver.</strong></p>

<p>Queue pairs (QP):</p>

<ol>
  <li>includes send queue (SQ) and receive queue (RQ)</li>
  <li>QP is mapped to the user defined process’ virtual memory space. And the process can access RNIC directly with this memory space.</li>
</ol>

<p>Worker Request (WR):</p>

<ol>
  <li>user can create Work request, WR includes operations sender want to send.</li>
  <li>WR is added to the QP.</li>
  <li>RDMA hardware will consume the WR.
    <ol>
      <li>read the message to be sent from buffer (specified in WR).</li>
      <li>Performs the operation and posts a work completion to CQ.</li>
    </ol>
  </li>
</ol>

<p>Complete Queue (CQ):  Infrom user that the specific WQ has been finished. one element in CQ can be corresponding to either SQ or RQ.</p>

<p>Both QPs and MRs can have different access modes (e.g., <strong>read-only or read-write)</strong>.</p>

<p>The <strong>same memory can be registered multiple times,</strong> yielding multiple MRs, each with its own access mode.In this way, different remote machines can have different access rights to the same memory</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220108220423100.png" alt="image-20220108220423100" /></p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220108221305246.png" alt="image-20220108221305246" /></p>

<h1 id="system-overview">System Overview</h1>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220109165502521.png" alt="image-20220109165502521" /></p>

<p>Replication plane: (A replica only has one of these components active, depending on its role in the system.)</p>

<ol>
  <li>Replicator: replicate a request from leader to followers</li>
  <li>Replayer: replays entries from log</li>
</ol>

<p>Background plane:</p>

<ol>
  <li>leader election: detect failure of leader and select other replicas to become leader</li>
  <li>Premission management: grants or revokes write access.</li>
</ol>

<p>Logging:  Store client requests to be replicated.</p>

<p>Leader:</p>

<ol>
  <li>The leader captures a client request, uses an RDMA Write to append that request to the log of each follower,</li>
  <li>Continues the application to process the request.</li>
</ol>

<p>Follower:</p>

<ol>
  <li>Each replica grants RDMA write premission to its log for its current leader only.</li>
  <li>Detect a new request in their log, they inject the request into the application, thereby updating the replicas.</li>
  <li>Monitor leader state to check it is still active.</li>
</ol>

<p>Handle Leader Failures</p>

<ol>
  <li>
    <p>Detect failures:</p>

    <p>leader periodically increments a local counter, and the follower perodically check the counter using RDMA Read.  if follower dont detect an increment of the counter after a few tries,  a new leader elected.</p>
  </li>
  <li>
    <p>New leader election</p>

    <p>New leader revokes a write premission by any old leader. such that old leader cannot interfere new leader.</p>
  </li>
</ol>

<h1 id="replication-plane">Replication Plane</h1>

<p>Safety: only a leader replica communicates over the network ensure safety.</p>

<h2 id="log-structure">Log Structure:</h2>

<ol>
  <li>minProposal: Each replica publishes a minProposal, which represent the minimum proposal number it can accept.</li>
  <li>FUO: first undecided offset, represent lowest log index</li>
  <li>each slot is (propNr, value) tuple</li>
</ol>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220110114417279.png" alt="image-20220110114417279" /></p>

<h2 id="basic-algorithm">Basic Algorithm</h2>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220110115051776.png" alt="image-20220110115051776" /></p>

<h2 id="extension-algorithm">Extension Algorithm</h2>

<p>After a replica becomes leader and establishes its confirmed followers, <strong>but before attempting to replicate new value</strong>, the leader brings itself and its follower <strong>up to data by copying</strong> the contents of more up-to-date log to less up-to-date log.</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220112105720809.png" alt="image-20220112105720809" /></p>

<h3 id="omitting-the-prepare-phase">Omitting the prepare phase</h3>

<p>Once a leader finds only empty slots at a given index at all of its confirmed followers at line 23, the leader may omit the prepare phase for higher indexes. With this optimization, the cost of a Propose call becomes a single RDMA write to a majority in the common case.</p>

<h3 id="growing-confirmed-followers">Growing confirmed followers</h3>

<p>Leader grows its confirmed followers set by briefly waiting for responses from all replicas during its initial request for permission.</p>

<h3 id="replayer">Replayer</h3>

<p>Follower continually monitor the log for new entries and check the extra canary byte to make sure the data read is complete before executing.</p>

<p>Followers can monitor their local logs and commit all values up to (but excluding) the highest non-empty log index.</p>

<h1 id="background--plane">Background  Plane</h1>

<h2 id="leader-election">Leader Election</h2>

<p>Replica A decides that B is leader if B is the replica <strong>with the lowest id</strong>, among those that A considers to be alive.</p>

<p>To know whether a replica has failed, we employ a <em>pull- score</em> mechanism, based on a <em>local heartbeat</em> counter. A leader election thread <strong>continually increments its own counter locally</strong> and <strong>uses RDMA Reads to read the counters (heart- beats) of other replicas and check whether they have been updated.</strong></p>

<p>Timeout Mechanism</p>

<p>A small timeout in our detection algorithm (scoring), and a longer timeout built into the RDMA connection mechanism.</p>

<p>Make sure both Replication and leader election threads works.</p>

<p>Every X=10000 iterations, the leader election thread checks the replication thread for activity. if replication thread is stuck, new leader will be elected.</p>

<h2 id="permission-management">Permission Management</h2>

<p>Each replica maintains the invariant that <strong>only one replica at a time has write permission on its log</strong>.</p>

<p>RDMA provides multiple mechanisms to grant and revoke write access.</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220112151122946.png" alt="image-20220112151122946" /></p>

<p>However, changing a QPs access flags while RDMA operations to that QP are in flight sometimes causes the QP to go into an error state. Therefore, in Mu they use a fast- slow path approach: they first optimistically try to change permissions using the faster QP access flag method and, if that leads to an error, switch to the slower, but robust, QP state method.</p>

<h2 id="log-recycling">Log Recycling</h2>

<p>Use limited memory to store logs.</p>

<ol>
  <li>the leader’s background plane reads the log heads of all followers and computes <em>minHead</em>,</li>
  <li>Log entries up to the minHead can be reused</li>
  <li>The leader zeroes all follower logs after the leader’s first undecided offset and before min- Head, using an RDMA Write per follower.</li>
</ol>

<h2 id="adding-and-removing-replicas">Adding and removing replicas</h2>

<p>Use consensus itself to inform replicas about the change.</p>

<p>Removing replica: it stops executing, while other replicas subsequently ignore any communication with it.</p>

<p>Adding replica: uses the standard approach of check-pointing state.</p>

<h1 id="implementation">Implementation</h1>

<p>7k C++ code,   Ibverbs (RDMA library), modular, QP exchange layer</p>

<h1 id="evaluation">Evaluation</h1>

<p>Measurements:</p>

<ol>
  <li>Replication latency (change with payload size, compare with other? )</li>
  <li>Fail-over time</li>
  <li>Throughput</li>
</ol>

<p>Evaluate on 4 nodes and compare with APUS, DARE, HERMES.</p>

<p>Application: three key-value stores (Redis, Memcached, HERD) , order matching engine for financial exchange.</p>

<h2 id="common-case-replication-latency">Common case replication latency</h2>

<h3 id="standalone-runs">Standalone runs</h3>

<ol>
  <li>No leader failure</li>
  <li>Standalone mode: run just replication layer without applications, leader generate random payload and use loop to propose.</li>
</ol>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220112153800784.png" alt="image-20220112153800784" /></p>

<p>Analysis</p>

<p>Standalone outperforms other due to processor cache effects.</p>

<p>In standalone runs, logs and queue pairs are alwasys in cache, and requests need not be fetched from memory.</p>

<h3 id="attached-runs">Attached runs:</h3>

<ol>
  <li>
    <p>with various appliactions</p>
  </li>
  <li>
    <p>Mu supports two ways of attaching to an application, direct or handover</p>

    <ol>
      <li>direct mode: same thread to run both application and replication share L1 and L2 caches.</li>
      <li>handover places application thread on separate core from replication thread, avoid shareing caches.</li>
    </ol>

    <p>Direct mode: used in large workloads applications.</p>

    <p>Handover places: used in lighter weight applications.</p>
  </li>
</ol>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220112161637165.png" alt="image-20220112161637165" /></p>

<p>Analysis:  (why others are slow?)</p>

<p>APUS and Hermes:  the need to involve the CPU of many replicas in the critical path.</p>

<p>DARE and APUS:  sequentializing several RDMA operations so that their variance aggregates.</p>

<h2 id="end-to-end-latency">End to end latency</h2>

<p>Includes the latency incurred by the application and by replication (if enabled).</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220112162121156.png" alt="image-20220112162121156" /></p>

<h2 id="failover-time---detection-time-premission-switch-time-">Failover time ( = detection time, premission switch time, )</h2>

<p>Repeatedly introduce leader failures by delaying the leader, making it become unresponsive (1k times). This causes other replicas to observe that the leader’s heartbeat has stopped changing, and thus detect a failure.</p>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220112163941698.png" alt="image-20220112163941698" /></p>

<p>Analysis:</p>

<p>Time to switch permissions (one to revoke write permission from the old leader and one to grant it to the new leader)  constitutes about 30% of the total fail-over time</p>

<p>The rest of the fail-over time is attributed to failure de- tection (≈600<em>μs</em>)</p>

<h2 id="throughput">Throughput</h2>

<p>Run a standalone microbenchmark.</p>

<p>Increase workloads (throughputs) in two ways:</p>

<ol>
  <li>by batching requests together before replicating,</li>
  <li>by allowing multiple out- standing requests at a time.</li>
</ol>

<p><img src="https://github.com/NLGithubWP/tech-notebook/raw/master/img/a_img_store/image-20220112164916316.png" alt="image-20220112164916316" /></p>

<p>Latency and throughput both increase as the batch size increases</p>

<p>Increasing the number of outstanding requests allowed while keeping the batch size constant substantially increases throughput at a small latency cost.</p>

:ET