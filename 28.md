**INFaaS: Automated** **Model-less** Inference Serving

# Introduction

## Problems

Inference query various on service level objectives such as accuracy, latency, and cost.

To better answer the user's inference request, the ML platform has to select the 

1. `Right model`:  RestNet50, VGG16 (TensorFlow, PyTorch)
2. `Model optimizations `: TensorRT, TVM, XLA. 
3. `Hyper-parameters`:  batch-size
4. `Suitable hardware platforms`: Haswell / SkyLake CPU, GPU, TPU, NPU.
5. `Auto-scaling configurations`

But the search space is huge. For example, 21 already-trained image classification models can generate 4032 model-variants on AWS-EC2 platform by:

- Applying various model graph optimizers (TensorRT, TVM, etc)
- Optimizing for different batch sizes.
- Changing underlying hardware resources. (CPU, GPU)

The 4032 model variants vary across different dimensions:

- accuracies range from 56.6% to 82.5%
- model loading latencies range from 590ms to 11s 
- inference latencies for a single query range from 1.5ms to 5.7s 
- Computational requirements range from 0.48 to 24 GFLOPS 
- The hardware cost of hardware ranges m $0.096/hr for 2 vCPUs to $3.06/hr for a V100 GPU

`Problem1: How to dynamically choose the right model variant among 4032 search spaces`?

Besides, traditional horizontal scaling reply on statically-fixed inference worker replication, which is insufficient due to 

1. Spawning a new VM incurs significant latency
2. the right variant may change with load.  Replication statically-fixed cannot meet the new requirement
3. Replication may have a large delay since new resource may not available,

`Problem2: How to scale more efficiently?`

When the load is low, idle model taking resources is a waste. 

`Problem3: how to improve resource utilization?`

## Contribution

The paper proposes INFaaS -  an automated model-less system for distributed inference serving. it can 

1. Selects the best model-invariant based on user-specified requirements like cost, accuracy, latency. 
2. Combine VM-level (horizontal scaling) and model-level `autoscaling` to dynamically react to changing applications and request patterns `such that the total cost is the lowest`. eg, multiple model-variants to serve one type of request instead of replicating one model-variant many times. 

![image-20220302161845052](imgs/image-20220302161845052.png)

3. Support multi-tenancy by sharing resources across applications and models when workload is low. When the load is low, the performance is almost the same. 

   The point when co-location starts affecting the performance varies across models and depends on both the load and the hardware architecture.

![image-20220302162948681](imgs/image-20220302162948681.png)

# INFaaS

**Design principles**

- Declarative API: developer only focuses on high-level `latency, cost, or accuracy` requirements.
-  INFaaS should automatically and efficiently select a model-variant for
  - Serve each query
  - Scale in reaction to changing application load (when the load is increasing?)
- Improve resource utilization.  resource sharing 
- the system is modular and extensible.

## Model inference 

**Model Registration**

User register trained-models ( ONNX Format ) with a validation-dataset (eg,. ResNet50) and assigns AppID, ModelID, ModelName. One application can include many models for different subtask

Backend test accuracy using validation-dataset

**Query submission**

User submit query with latency, cost, accuracy (eg, . Latency = 200ms,  accuracy >=70%). And then the backend searches model variants and `scaling strategies.` 

![image-20220302170229107](imgs/image-20220302170229107.png)

## Architecture





# Selecting and Scaling Model Variants



# Implementation



# Evaluation





